import os
import streamlit as st
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.indexes import VectorstoreIndexCreator
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader

print("Import Complete")

# ç’°å¢ƒãƒ‘ã‚¹ã®è¨­å®š
load_dotenv()

# APIã‚­ãƒ¼ã®ç™»éŒ²
api_key = os.getenv("API_KEY")
os.environ["OPENAI_API_KEY"] = api_key

@st.cache_resource  # ã“ã‚Œã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦å†è¨ˆç®—ã‚’é˜²ãï¼
def load_index():
    print("ğŸ”„ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ...")
    
    file_path = './data.txt'
    loader = TextLoader(file_path, encoding='utf-8')
    
    text_splitter = CharacterTextSplitter(
        separator="\n\n",
        chunk_size=150,
        chunk_overlap=0,
    )

    embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

    index = VectorstoreIndexCreator(
        vectorstore_cls=Chroma,
        embedding=embedding,
        text_splitter=text_splitter
    ).from_loaders([loader])

    print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ä½œæˆå®Œäº†")
    return index

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸçŠ¶æ…‹ã§ãƒ­ãƒ¼ãƒ‰
index = load_index()

# LLMã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
@st.cache_resource
def get_llm():
    return ChatOpenAI(model_name="gpt-4o-mini", temperature=0, max_tokens=300)

llm = get_llm()

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ä¸­å³¶å®¶bot")

user_question = st.text_input("è³ªå•", value="")

if st.button("é€ä¿¡"):
    if user_question.strip():
        search_results = index.query(user_question, llm=llm)
        
        template = """
        ä»¥ä¸‹ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã€æ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚ç®‡æ¡æ›¸ãã ã‘ã§å¤§ä¸ˆå¤«ã§ã™ã€‚
        çŸ¥è­˜ï¼š{search_results}
        è³ªå•ï¼š{user_question}
        """
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ã‚ãªãŸã¯çŒ«ã§ã€èªå°¾ã«ã€Œã«ã‚ƒã‚“ã€ã¨ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™"),
            ("user", template)
        ])

        # å‡ºåŠ›ç”¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        output_parser = StrOutputParser()

        # ãƒã‚§ãƒ¼ãƒ³ã®ä½œæˆ
        chain = prompt | llm | output_parser

        # ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ
        response = chain.invoke({
            "search_results": search_results,
            "user_question": user_question
        })

        st.write(response)
